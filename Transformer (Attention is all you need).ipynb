{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.utils.tensorboard import SummaryWriter\nimport math\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom pathlib import Path\nfrom typing import Any\nfrom tqdm import tqdm\nimport warnings\n\nwarnings.filterwarnings('ignore')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-26T18:09:58.675390Z","iopub.execute_input":"2024-06-26T18:09:58.676212Z","iopub.status.idle":"2024-06-26T18:10:14.761198Z","shell.execute_reply.started":"2024-06-26T18:09:58.676178Z","shell.execute_reply":"2024-06-26T18:10:14.760232Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-26 18:10:04.213617: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-26 18:10:04.213748: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-26 18:10:04.377094: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"class InputEmbeddings(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int):\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        \n    def forward(self, x):\n        return self.embedding(x) * math.sqrt(self.d_model)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:14.762950Z","iopub.execute_input":"2024-06-26T18:10:14.763480Z","iopub.status.idle":"2024-06-26T18:10:14.771408Z","shell.execute_reply.started":"2024-06-26T18:10:14.763454Z","shell.execute_reply":"2024-06-26T18:10:14.770520Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.seq_len = seq_len\n        self.dropout = nn.Dropout(dropout)\n        \n        pe = torch.zeros(seq_len, d_model) \n        \n        position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1)\n        \n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        pe = pe.unsqueeze(0)\n        \n        self.register_buffer('pe', pe) \n        \n    def forward(self,x):\n        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:14.772853Z","iopub.execute_input":"2024-06-26T18:10:14.773251Z","iopub.status.idle":"2024-06-26T18:10:14.806029Z","shell.execute_reply.started":"2024-06-26T18:10:14.773220Z","shell.execute_reply":"2024-06-26T18:10:14.805205Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n    def __init__(self, eps: float = 10**-6) -> None: \n        super().__init__()\n        self.eps = eps\n        \n        self.alpha = nn.Parameter(torch.ones(1))\n        \n        self.bias = nn.Parameter(torch.zeros(1))\n        \n    def forward(self, x):\n        mean = x.mean(dim = -1, keepdim = True)\n        std = x.std(dim = -1, keepdim = True)        \n        return self.alpha * (x-mean) / (std + self.eps) + self.bias","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:14.807348Z","iopub.execute_input":"2024-06-26T18:10:14.807740Z","iopub.status.idle":"2024-06-26T18:10:14.814963Z","shell.execute_reply.started":"2024-06-26T18:10:14.807709Z","shell.execute_reply":"2024-06-26T18:10:14.814135Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class FeedForwardBlock(nn.Module):    \n    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n        super().__init__()\n        self.linear_1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(d_ff, d_model)\n        \n    def forward(self, x):\n        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:14.817420Z","iopub.execute_input":"2024-06-26T18:10:14.817747Z","iopub.status.idle":"2024-06-26T18:10:14.825572Z","shell.execute_reply.started":"2024-06-26T18:10:14.817715Z","shell.execute_reply":"2024-06-26T18:10:14.824876Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttentionBlock(nn.Module):    \n    def __init__(self, d_model: int, h: int, dropout: float) -> None: # h = number of heads\n        super().__init__()\n        self.d_model = d_model\n        self.h = h\n        \n        assert d_model % h == 0, 'd_model is not divisible by h'\n        \n        self.d_k = d_model // h\n        \n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n        self.w_o = nn.Linear(d_model, d_model)\n        \n        self.dropout = nn.Dropout(dropout) \n        \n    \n    @staticmethod\n    def attention(query, key, value, mask, dropout: nn.Dropout): \n        \n        d_k = query.shape[-1] \n        \n        attention_scores = (query @ key.transpose(-2,-1)) / math.sqrt(d_k) \n        \n        if mask is not None:  \n            attention_scores.masked_fill_(mask == 0, -1e9)\n        attention_scores = attention_scores.softmax(dim = -1)\n        if dropout is not None:\n            attention_scores = dropout(attention_scores)\n            \n        return (attention_scores @ value), attention_scores\n        \n    def forward(self, q, k, v, mask): \n        query = self.w_q(q)\n        key = self.w_k(k)\n        value = self.w_v(v)\n\n        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n        \n        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n        \n        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n        \n        return self.w_o(x) ","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:14.826534Z","iopub.execute_input":"2024-06-26T18:10:14.826788Z","iopub.status.idle":"2024-06-26T18:10:14.839142Z","shell.execute_reply.started":"2024-06-26T18:10:14.826768Z","shell.execute_reply":"2024-06-26T18:10:14.838424Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class ResidualConnection(nn.Module):\n    def __init__(self, dropout: float) -> None:\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.norm = LayerNormalization() \n    \n    def forward(self, x, sublayer):\n        return x + self.dropout(sublayer(self.norm(x))) ","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:14.840189Z","iopub.execute_input":"2024-06-26T18:10:14.840449Z","iopub.status.idle":"2024-06-26T18:10:14.849686Z","shell.execute_reply.started":"2024-06-26T18:10:14.840422Z","shell.execute_reply":"2024-06-26T18:10:14.848839Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n        \n    def forward(self, x, src_mask):\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n        \n        x = self.residual_connections[1](x, self.feed_forward_block)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:14.850776Z","iopub.execute_input":"2024-06-26T18:10:14.851080Z","iopub.status.idle":"2024-06-26T18:10:14.860564Z","shell.execute_reply.started":"2024-06-26T18:10:14.851058Z","shell.execute_reply":"2024-06-26T18:10:14.859711Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, layers: nn.ModuleList) -> None:\n        super().__init__()\n        self.layers = layers\n        self.norm = LayerNormalization()\n        \n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:14.861715Z","iopub.execute_input":"2024-06-26T18:10:14.862253Z","iopub.status.idle":"2024-06-26T18:10:14.869238Z","shell.execute_reply.started":"2024-06-26T18:10:14.862223Z","shell.execute_reply":"2024-06-26T18:10:14.868474Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n    def __init__(self,  self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.cross_attention_block = cross_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n        \n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n        \n        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n        \n        x = self.residual_connections[2](x, self.feed_forward_block)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:14.870186Z","iopub.execute_input":"2024-06-26T18:10:14.870729Z","iopub.status.idle":"2024-06-26T18:10:14.878710Z","shell.execute_reply.started":"2024-06-26T18:10:14.870706Z","shell.execute_reply":"2024-06-26T18:10:14.877918Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, layers: nn.ModuleList) -> None:\n        super().__init__()\n        self.layers = layers\n        self.norm = LayerNormalization()\n        \n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, encoder_output, src_mask, tgt_mask)\n        return self.norm(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:14.879645Z","iopub.execute_input":"2024-06-26T18:10:14.879921Z","iopub.status.idle":"2024-06-26T18:10:14.890135Z","shell.execute_reply.started":"2024-06-26T18:10:14.879899Z","shell.execute_reply":"2024-06-26T18:10:14.889210Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class ProjectionLayer(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int) -> None:\n        super().__init__()\n        self.proj = nn.Linear(d_model, vocab_size)\n    def forward(self, x):\n        return torch.log_softmax(self.proj(x), dim = -1)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:14.891317Z","iopub.execute_input":"2024-06-26T18:10:14.892095Z","iopub.status.idle":"2024-06-26T18:10:14.897777Z","shell.execute_reply.started":"2024-06-26T18:10:14.892064Z","shell.execute_reply":"2024-06-26T18:10:14.896930Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.src_pos = src_pos\n        self.tgt_pos = tgt_pos\n        self.projection_layer = projection_layer\n        \n    def encode(self, src, src_mask):\n        src = self.src_embed(src)\n        src = self.src_pos(src)\n        return self.encoder(src, src_mask)\n    \n    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n        tgt = self.tgt_embed(tgt) \n        tgt = self.tgt_pos(tgt) \n        \n        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n    \n    def project(self, x):\n        return self.projection_layer(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:14.898902Z","iopub.execute_input":"2024-06-26T18:10:14.899174Z","iopub.status.idle":"2024-06-26T18:10:14.907161Z","shell.execute_reply.started":"2024-06-26T18:10:14.899153Z","shell.execute_reply":"2024-06-26T18:10:14.906396Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n    \n    src_embed = InputEmbeddings(d_model, src_vocab_size) \n    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size) \n    \n    src_pos = PositionalEncoding(d_model, src_seq_len, dropout) \n    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout) \n    \n    encoder_blocks = []  \n    for _ in range(N): \n        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)  \n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)  \n        \n        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n        encoder_blocks.append(encoder_block)\n        \n    decoder_blocks = [] \n    for _ in range(N): \n        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n        \n        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n        decoder_blocks.append(decoder_block)\n        \n    encoder = Encoder(nn.ModuleList(encoder_blocks))\n    decoder = Decoder(nn.ModuleList(decoder_blocks))\n    \n    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n    \n    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n    \n    for p in transformer.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_(p)\n            \n    return transformer ","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:14.910550Z","iopub.execute_input":"2024-06-26T18:10:14.910866Z","iopub.status.idle":"2024-06-26T18:10:14.921563Z","shell.execute_reply.started":"2024-06-26T18:10:14.910837Z","shell.execute_reply":"2024-06-26T18:10:14.920733Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def build_tokenizer(config, ds, lang):\n    \n    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n    \n    if not Path.exists(tokenizer_path): \n        \n        tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]')) \n        tokenizer.pre_tokenizer = Whitespace() \n        \n        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\", \n                                                     \"[SOS]\", \"[EOS]\"], min_frequency = 2) \n        \n        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer = trainer)\n        tokenizer.save(str(tokenizer_path)) \n    else:\n        tokenizer = Tokenizer.from_file(str(tokenizer_path)) \n    return tokenizer ","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:14.922614Z","iopub.execute_input":"2024-06-26T18:10:14.922911Z","iopub.status.idle":"2024-06-26T18:10:14.932210Z","shell.execute_reply.started":"2024-06-26T18:10:14.922889Z","shell.execute_reply":"2024-06-26T18:10:14.931389Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def get_all_sentences(ds, lang):\n    for pair in ds:\n        yield pair['translation'][lang]","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:14.933123Z","iopub.execute_input":"2024-06-26T18:10:14.933392Z","iopub.status.idle":"2024-06-26T18:10:14.941680Z","shell.execute_reply.started":"2024-06-26T18:10:14.933370Z","shell.execute_reply":"2024-06-26T18:10:14.940928Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def get_ds(config):\n    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split = 'train') \n    \n    tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n    tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n    \n    train_ds_size = int(0.9 * len(ds_raw)) \n    val_ds_size = len(ds_raw) - train_ds_size \n    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n                                    \n    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n                                    \n    max_len_src = 0\n    max_len_tgt = 0\n    for pair in ds_raw:\n        src_ids = tokenizer_src.encode(pair['translation'][config['lang_src']]).ids\n        tgt_ids = tokenizer_src.encode(pair['translation'][config['lang_tgt']]).ids\n        max_len_src = max(max_len_src, len(src_ids))\n        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n        \n    print(f'Max length of source sentence: {max_len_src}')\n    print(f'Max length of target sentence: {max_len_tgt}')\n    \n    train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle = True)\n    val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)\n    \n    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt ","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:14.942731Z","iopub.execute_input":"2024-06-26T18:10:14.942989Z","iopub.status.idle":"2024-06-26T18:10:14.952033Z","shell.execute_reply.started":"2024-06-26T18:10:14.942967Z","shell.execute_reply":"2024-06-26T18:10:14.951273Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def casual_mask(size):\n    mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n    return mask == 0","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:14.952910Z","iopub.execute_input":"2024-06-26T18:10:14.953178Z","iopub.status.idle":"2024-06-26T18:10:14.962506Z","shell.execute_reply.started":"2024-06-26T18:10:14.953150Z","shell.execute_reply":"2024-06-26T18:10:14.961726Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class BilingualDataset(Dataset):\n    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n        super().__init__()\n        \n        self.seq_len = seq_len\n        self.ds = ds\n        self.tokenizer_src = tokenizer_src\n        self.tokenizer_tgt = tokenizer_tgt\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n        \n        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n\n        \n    def __len__(self):\n        return len(self.ds)\n    \n    def __getitem__(self, index: Any) -> Any:\n        src_target_pair = self.ds[index]\n        src_text = src_target_pair['translation'][self.src_lang]\n        tgt_text = src_target_pair['translation'][self.tgt_lang]\n        \n        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n        \n\n        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 \n\n        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n        \n        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n            raise ValueError('Sentence is too long')\n         \n        encoder_input = torch.cat(\n            [\n            self.sos_token,\n            torch.tensor(enc_input_tokens, dtype = torch.int64), \n            self.eos_token, # Inserting the '[EOS]' token\n            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n            ]\n        )\n        \n        decoder_input = torch.cat(\n            [\n                self.sos_token,\n                torch.tensor(dec_input_tokens, dtype = torch.int64),\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64)\n            ]\n        )\n        \n        label = torch.cat(\n            [\n                torch.tensor(dec_input_tokens, dtype = torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64)\n            ]\n        )\n        \n        assert encoder_input.size(0) == self.seq_len\n        assert decoder_input.size(0) == self.seq_len\n        assert label.size(0) == self.seq_len\n        \n        return {\n            'encoder_input': encoder_input,\n            'decoder_input': decoder_input, \n            'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)), \n            'label': label,\n            'src_text': src_text,\n            'tgt_text': tgt_text\n        }    \n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:14.963712Z","iopub.execute_input":"2024-06-26T18:10:14.963982Z","iopub.status.idle":"2024-06-26T18:10:14.978715Z","shell.execute_reply.started":"2024-06-26T18:10:14.963958Z","shell.execute_reply":"2024-06-26T18:10:14.977906Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n    \n    encoder_output = model.encode(source, source_mask)\n    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n    \n    while True:\n        if decoder_input.size(1) == max_len:\n            break\n            \n        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n        \n        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n        \n        prob = model.project(out[:, -1])\n        \n        _, next_word = torch.max(prob, dim=1)\n        decoder_input = torch.cat([decoder_input, torch.empty(1,1). type_as(source).fill_(next_word.item()).to(device)], dim=1)\n        \n        if next_word == eos_idx:\n            break\n            \n    return decoder_input.squeeze(0)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:14.979711Z","iopub.execute_input":"2024-06-26T18:10:14.979969Z","iopub.status.idle":"2024-06-26T18:10:14.989483Z","shell.execute_reply.started":"2024-06-26T18:10:14.979947Z","shell.execute_reply":"2024-06-26T18:10:14.988750Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n    model.eval()\n    count = 0 \n    console_width = 80 \n    \n    with torch.no_grad(): \n        for batch in validation_ds:\n            count += 1\n            encoder_input = batch['encoder_input'].to(device)\n            encoder_mask = batch['encoder_mask'].to(device)\n            \n            assert encoder_input.size(0) ==  1, 'Batch size must be 1 for validation.'\n            \n            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n            \n            source_text = batch['src_text'][0]\n            target_text = batch['tgt_text'][0] \n            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) \n            \n            print_msg('-'*console_width)\n            print_msg(f'SOURCE: {source_text}')\n            print_msg(f'TARGET: {target_text}')\n            print_msg(f'PREDICTED: {model_out_text}')\n            \n            if count == num_examples:\n                break","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:14.990421Z","iopub.execute_input":"2024-06-26T18:10:14.990720Z","iopub.status.idle":"2024-06-26T18:10:15.000405Z","shell.execute_reply.started":"2024-06-26T18:10:14.990697Z","shell.execute_reply":"2024-06-26T18:10:14.999494Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def get_model(config, vocab_src_len, vocab_tgt_len):\n    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:15.001432Z","iopub.execute_input":"2024-06-26T18:10:15.001777Z","iopub.status.idle":"2024-06-26T18:10:15.011465Z","shell.execute_reply.started":"2024-06-26T18:10:15.001753Z","shell.execute_reply":"2024-06-26T18:10:15.010638Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def get_config():\n    return{\n        'batch_size': 8,\n        'num_epochs': 20,\n        'lr': 10**-4,\n        'seq_len': 350,\n        'd_model': 512,\n        'lang_src': 'en',\n        'lang_tgt': 'it',\n        'model_folder': 'weights',\n        'model_basename': 'tmodel_',\n        'preload': None,\n        'tokenizer_file': 'tokenizer_{0}.json',\n        'experiment_name': 'runs/tmodel'\n    }\n    \n\ndef get_weights_file_path(config, epoch: str):\n    model_folder = config['model_folder'] \n    model_basename = config['model_basename'] \n    model_filename = f\"{model_basename}{epoch}.pt\"\n    return str(Path('.')/ model_folder/ model_filename) ","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:15.012616Z","iopub.execute_input":"2024-06-26T18:10:15.012887Z","iopub.status.idle":"2024-06-26T18:10:15.020037Z","shell.execute_reply.started":"2024-06-26T18:10:15.012865Z","shell.execute_reply":"2024-06-26T18:10:15.019307Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def train_model(config):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device {device}\")\n    \n    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n    \n    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n    \n    model = get_model(config,tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n    \n    writer = SummaryWriter(config['experiment_name'])\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps = 1e-9)\n    \n    initial_epoch = 0\n    global_step = 0\n    \n    if config['preload']:\n        model_filename = get_weights_file_path(config, config['preload'])\n        print(f'Preloading model {model_filename}')\n        state = torch.load(model_filename) \n        \n        initial_epoch = state['epoch'] + 1\n        optimizer.load_state_dict(state['optimizer_state_dict'])\n        global_step = state['global_step']\n        \n    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id('[PAD]'), label_smoothing = 0.1).to(device)\n    \n    \n    for epoch in range(initial_epoch, config['num_epochs']):\n        \n        batch_iterator = tqdm(train_dataloader, desc = f'Processing epoch {epoch:02d}')\n        \n        for batch in batch_iterator:\n            model.train()\n            \n            encoder_input = batch['encoder_input'].to(device)\n            decoder_input = batch['decoder_input'].to(device)\n            encoder_mask = batch['encoder_mask'].to(device)\n            decoder_mask = batch['decoder_mask'].to(device)\n            \n            encoder_output = model.encode(encoder_input, encoder_mask)\n            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n            proj_output = model.project(decoder_output)\n            \n            label = batch['label'].to(device)\n            \n            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n            \n            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n            \n            writer.add_scalar('train loss', loss.item(), global_step)\n            writer.flush()\n            \n            loss.backward()\n            \n            optimizer.step()\n            \n            optimizer.zero_grad()\n            \n            global_step += 1\n            \n\n        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n         \n        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'global_step': global_step\n        }, model_filename)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:15.021207Z","iopub.execute_input":"2024-06-26T18:10:15.021461Z","iopub.status.idle":"2024-06-26T18:10:15.035885Z","shell.execute_reply.started":"2024-06-26T18:10:15.021426Z","shell.execute_reply":"2024-06-26T18:10:15.035040Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"config = get_config()\ntrain_model(config)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T18:10:15.036918Z","iopub.execute_input":"2024-06-26T18:10:15.037182Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Using device cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/28.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc227f77463d4070b67dfd9457396cf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/5.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fd7444f26ae4701ac7d218907dece28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/32332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"860e86aaf9b747fd8004b5c7198e7b72"}},"metadata":{}},{"name":"stdout","text":"Max length of source sentence: 309\nMax length of target sentence: 274\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch 00:  49%|████▉     | 1795/3638 [07:36<07:48,  3.94it/s, loss=5.582]","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}